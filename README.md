# RARE-Research-Agent-with-Retrieval-and-Explaination

Got it! Here's the full **`README.md`** formatted properly for your project, renamed to **RARE** (Research Agent with Retrieval and Explanation), ready to save as `README.md`:

````markdown
# RARE: Research Agent with Retrieval and Explanation

RARE is a single-file **CLI agent** for retrieval-augmented generation (RAG). It allows you to:

- Index PDFs and text documents into a Chroma vector store.
- Query documents using a large language model (LLM) with retrieval.
- Interactively chat with your document knowledge base.
- Support multiple LLM backends: Hugging Face Hub, LM Studio HTTP, or local Transformers.

---

## üì¶ Features

- **Document indexing:** Split PDFs, TXT, or Markdown files into chunks and embed them using `SentenceTransformer`.
- **Retrieval QA:** Ask questions and get answers with source snippets.
- **Conversational chat:** Multi-turn chat with memory using `ConversationalRetrievalChain`.
- **Multiple LLM backends:**
  - Hugging Face Hub (`HuggingFaceEndpoint`)
  - LM Studio HTTP (`LMStudioHTTP`)
  - Local Transformers (`AutoModelForCausalLM` + `transformers` pipeline)

---

## ‚öôÔ∏è Requirements

- Python 3.10+
- pip packages (can be installed via `requirements.txt`):

```bash
pip install -r requirements.txt
````

**Requirements include:**

* `typer`
* `langchain`
* `langchain-community`
* `pypdf`
* `transformers`
* `torch`
* `chromadb`
* `python-dotenv`
* `requests`
* `sentence-transformers`

---

## üìù Setup

1. **Clone the repo**

```bash
git clone https://github.com/yourusername/rare_agent.git
cd rare_agent
```

2. **Create `.env` for API keys (optional)**

```bash
touch .env
```

Add your keys if using Hugging Face Hub or LM Studio:

```dotenv
HF_API_TOKEN=<your_huggingface_token>
LMSTUDIO_API_KEY=<your_lmstudio_api_key>
```

3. **Create `config.yaml`**

Example configuration:

```yaml
backend: huggingface_hub           # huggingface_hub | local_transformers | lmstudio
hf_model: EleutherAI/gpt-neo-2.7B  # only for huggingface_hub
local_model_name: EleutherAI/gpt-neo-2.7B
local_max_new_tokens: 512
lmstudio_url: http://127.0.0.1:8080/generate
lmstudio_api_key: ""
embedding_model: sentence-transformers/all-MiniLM-L6-v2
docs_dir: ./data/docs
persist_directory: ./chroma_db
collection_name: rare_collection
chunk_size: 800
chunk_overlap: 150
top_k: 4
temperature: 0.7
max_tokens: 512
```

---

## üìÇ Preparing Documents

Place PDFs, `.txt` or `.md` files under:

```text
./data/docs
```

The agent will automatically split and embed them for retrieval.

---

## üöÄ Usage

### 1. Index documents

```bash
python rare_agent.py index
```

Creates embeddings and stores them in the Chroma vector database (`persist_directory`).

---

### 2. Single-shot query

```bash
python rare_agent.py query "What is the safety policy?"
```

The CLI will return the answer along with source snippets.

---

### 3. Interactive chat

```bash
python rare_agent.py chat
```

Start a multi-turn conversation with your documents. Type `exit` or `quit` to leave.

---

## ‚öôÔ∏è Backend Options

| Backend              | Notes                                                  |
| -------------------- | ------------------------------------------------------ |
| `huggingface_hub`    | Requires `HF_API_TOKEN` and Hugging Face model repo.   |
| `local_transformers` | Loads a local Transformers model (`torch` compatible). |
| `lmstudio`           | HTTP endpoint compatible with LM Studio API.           |

---

## üß© Example `.env`:

```dotenv
HF_API_TOKEN=hf_xxxxx
LMSTUDIO_API_KEY=lm_xxxxx
```

---

## üîß Troubleshooting

1. **VectorStore not found:**
   Ensure you have indexed documents with `python rare_agent.py index`.

2. **GPU not detected:**
   Local Transformers backend will default to CPU if CUDA is not available.

3. **Hugging Face API errors:**
   Check your token and model repo name. Free-tier models have rate limits.

4. **LM Studio HTTP errors:**
   Ensure the endpoint URL is correct and the server is running.

---

## üìñ Notes

* Chroma vector store persists locally, so you can reuse embeddings without re-indexing.
* RARE is ideal for research scenarios where you want **answers backed by documents**.
* Supports chunked retrieval with memory for multi-turn conversations.

---

## üîó References

* [LangChain](https://www.langchain.com/)
* [Chroma Vector DB](https://www.trychroma.com/)
* [Hugging Face Models](https://huggingface.co/models)
* [LM Studio](https://lmstudio.ai/)
* [Transformers](https://huggingface.co/docs/transformers/)

```

---

If you want, I can also generate a **`requirements.txt`** and a **folder structure diagram** for RARE to make it fully plug-and-play for anyone.  

Do you want me to do that?
```
